created: 26.3.18 by Yarden


Gadi 14/5/18  add: 
  1) AMI data is split by kaldi scripts into: train , 
	Dev (for tuning-validation of the kaldi decoding system),and
	Eval (used as test). a Lexicon is also created (or used) by Kaldi.
 
	the train is further decomposed into 90% train and 10% validation for the use of the RNNLM.
  2) RNNLM_data_prep:
	- split training into 90% training and 10% validation (different from the validation(Dev) used  by Kaldi
	- prepares List_of_words (9999 words) which are the 9999 most frequent words in the train 
	  (in practice, the train contains only about 9200 unique words so all these 9200 words are taken + 800 taken from the lexicon:
		To generate the List, the lexicon is used: 1st: duplicates are removed, 2nd: countfrequency of lexicon word in train, sort by frequency, take 9999 first words. in AMI this will take 800 words from lexicon with 0 frequency)
	- prepare also a list of lexicon words that are out of the list words (OOL)  (unknown list)
	- prepares also  unk_prob which counts the frequency that the OOL  words appear in the the training (adding 1 for smoothing).
	  (in practice, all training AMI words appear in the list, nothing in the unk, therefore all unk probs are 1 
	  causing uniform distribution of the probabilities when computing the probs of an Out of list word )
	 
	 Theory about Kaldi rescoring ???): how Kaldi computes re-scoring of a chain of words: It should multiply the probs of words given 
	 the previous words. the RNNLM compute the prob of a word in the list.
	 if a word X in the chain is OOL (out of list), the RNNLM returns the prob of <OOL> token.
	 this probability should be divided in proportion to the unk_prob file (in practice in the AMI case, uniform distribution)
	
	


1) rnnlm_data_prep.sh:

 Input text file is: train_text=data/ihm/train/text.
 We also have nwords=9999 and heldout_sent=10000 so we can change the list size,
 but that would probably require some code changes.
 
 In our case, for the following - dir=data/vanilla_tensorflow
 
 This script prepares the data directory used for TensorFlow based RNNLM training
 it prepares the following files in the output-directory
 1. $dir/wordlist.rnn.final : wordlist for RNNLM
    format of this file is like the following:
        0   The
        1   a
        2   is
        ....
    note that we don't reserve the 0 id for any special symbol
 2. $dir/{train/valid} : the text files, with each sentence in a line

 3. $dir/unk.probs : this file provides information for distributing OOS probs
    among all the OOS words, in rnnlm-rescoring.  If provided, the
    probability for <OOS> would be proportionally distributed among all OOS words
    It is called unk.probs to be consistent with rnnlm-rescoring scripts with
    Mikolov's and Yandex's toolkits, but you could simply provide the count instead, as
    the binary would auto-normalize the counts into probabilities
    the format of this file is like the following:
         some-rare-word-1  0.0003
         some-rare-word-2  0.0004
         ...

 * In this baseline settings, the train set (about 77h) is divided to rnn-train and rnn-validation sets (about 90% and 10% accordingly).
   The word list is constructed from the N-gram LM lexicon (In our case was trained on AMI and Fisher databases).
   10,000 most frequent words in the rnn-train set is then narrowed down to be used as the rnn word list.
   Word frequency is determined by the unigram counts of the every lexicon-word in the rnn-train set.
   Notice that unigrams are actually the counts of words plus one, so word with count of one never appeared in the text.
   Also in this database (77h AMI train set) vocabulary is small and with a word list with the length of 10k words,    there are no out of list words occurrences.
   
2) next we run steps/tfrnnlm/vanilla_rnnlm.py with the following configurations:
   class Config(object):
   """Small config."""
   init_scale = 0.1
   learning_rate = 0.2
   max_grad_norm = 1
   num_layers = 1
   num_steps = 20
   hidden_size = 200
   max_epoch = 4
   max_max_epoch = 20
   keep_prob = 1
   lr_decay = 0.95
   batch_size = 64
   
   We can also use lstm_fast.py that have the following configurations:
   class Config(object):
   """Small config."""
   init_scale = 0.1
   learning_rate = 1
   max_grad_norm = 5
   num_layers = 2
   num_steps = 20
   hidden_size = 200
   max_epoch = 4
   max_max_epoch = 13
   keep_prob = 1.0
   lr_decay = 0.8
   batch_size = 64
   
   lstm_fast.py adds new_softmax - this new "softmax" function we show can train a "self-normalized" RNNLM where the sum of the output is automatically (close to) 1.0
   which saves a lot of computation for lattice-rescoring
   other changes that support lstm are also in the file.
  
   note: tensorflow flag parsing is only supported correctly in tensorflow 1.4 or below, so we (noam) added patch code to attend this error, lines 309-313 in vanilla_rnnlm.py (start of main function) those are not yet written in lstm_fast.py (26.3.18)
  
3) reader.py  -
   used to format input data.
   
   rnnlm_raw_data:
   Load RNNLM raw data from data directory "data_path".
   Args:
    data_path: string path to the directory where train/valid files are stored
   Returns:
    tuple (train_data, valid_data, test_data, vocabulary)
	train_data, valid_data and test_data are presented as list of indexes.
    where each of the data objects can be passed to RNNLMIterator.
   
   *!!! no test_data returned from rnnlm_raw_data for some reason!! 09/04/18
   
   
   rnnlm_producer:
   Iterate on the raw RNNLM data.

   This chunks up raw_data into batches of examples and returns Tensors that
   are drawn from these batches.

   Args:
    raw_data: one of the raw data outputs from rnnlm_raw_data.
    batch_size: int, the batch size.
    num_steps: int, the number of unrolls.
    name: the name of this operation (optional).

   Returns:
    A pair of Tensors, each shaped [batch_size, num_steps]. The second element
    of the tuple is the same data time-shifted to the right by one.

   Raises:
    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.
   
    data which is the raw data sliced up chunks of raw_data is a tensor composed with batch_size batches
    as seen in lines 86-89:
    data_len = tf.size(raw_data)
    batch_len = data_len // batch_size
    data = tf.reshape(raw_data[0 : batch_size * batch_len], [batch_size, batch_len])
   
    It then returns x,y tuple which is the same tensor of words indexes time shifted to the right by one.
    For further use x will be used as the input data and y will be used as the targets. 
   
	Creation of the graph part that slices batches of words as inputs for the rnn is in lines 101 - 107: 
	i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue() # creates a queue of integers representing slices of data in the data list. 
	x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps]) # returns a tensor which is a slice of the data in the size of batch_sizeXnum_steps - note that the returned tensor is in shape of [?,?] - for that the next line is required.
	x.set_shape([batch_size, num_steps])
	y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1])
	y.set_shape([batch_size, num_steps]) # does the same for y with 1 step time shift.
   
	note: tf.train.range_input_producer's output where shuffle=False is serial for each epoch.   
	
   
4) vanilla_rnnlm.py/lstm_fast.py:
   RnnlmModel:
	batch_size - number of example to be fed to RNN, default 64.
	num_steps - number of recurrent steps, default 20.
	size - size of layer, default 200
	vocab_size - RNN word list size, default 10000
	
   run_epoch:
	Here a single step of weights update is been calculated.
   
   Lines 266-271:
    vals = session.run(fetches, feed_dict) # run one step of TF computation and return new weights in vals["final state"] and the cost in vals["cost"].
    cost = vals["cost"]
    state = vals["final_state"] # update state to be the last calculated state for next step.

    costs += cost # summing the cost for average over iterations calculation 
    iters += model.input.num_steps
	
   Line 323:
    tf.train.Supervisor - creates Coordinator, saver and sessionManager for training.
	tf.train.range_input_producer from reader.py needs the Coordinator to work correctly.
	
5) run_lstm_fast.sh/run_lstm_fast_tests.sh/run_lstm_fast_WER_test.sh
   run_lstm_fast.sh - source script used to rescore the lattice with the TFRNNLM.
   run_lstm_fast_tests.sh and run_lstm_fast_WER_test.sh, both based on run_lstm_fast.sh.
   
   1st step Data prep by script:  local/tfrnnlm/rnnlm_data_prep.sh  - 
	Divide AMI train data to train and validation sets (about 90% and 10% accordingly).
	Counts unigrams on 90% train set and creates wordlist.rnn from the 10,000 most frequent words in the lexicon.
    (Lexicon taken from AMI-FISHER trained LM).
	
   2nd step is training the TFRNNLM using steps/tfrnnlm/lstm_fast_new_edit.py or steps/tfrnnlm/lstm_fast.py.
   Yarden wrote a .sh that generates several configuration files (different hyper params) and then runs new_edits program knows how to read configurations and runs lstmfast:
   In steps/tfrnnlm/lstm_fast_new_edit.py and run_lstm_fast_tests.sh configuration file is first created and than steps/tfrnnlm/lstm_fast_new_edit.py reads it so hyper parameters are easy to set. 
	run_lstm_fast_tests.sh can iterates different hyper parameters in order to compare perplexity.
	
	Another script runs lstm_newEdits for training a model and use it for rescoring (running another script)
	In run_lstm_fast_WER_test.sh we finally run steps/tfrnnlm/lmrescore_rnnlm_lat_pruned.sh to decode development and evaluation texts and get WER measurements.
   
		
   
   
   